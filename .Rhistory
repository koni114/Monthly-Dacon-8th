R.version
R.version
installr::check.for.updates.R()
installr::install.R()
updateR()
library(installr)
installr::updateR()
installr::updateR()
library(installr)
installr::updateR()
install.packages(c("clusterGeneration", "htmlwidgets", "igraph", "ps", "psych", "questionr", "RcppArmadillo", "readr", "recipes", "rJava", "rlang", "roxygen2", "rpart.plot", "RSQLite", "sandwich", "seriation", "sp", "SQUAREM", "stringi", "survival", "sys", "tibble", "tidyr", "TTR", "usethis", "vcd", "vctrs", "withr", "xfun", "XML", "xts"))
##############
## LightGBM ##
##############
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0rc1/lightgbm-3.0.0-1-r40-windows.zip"
local_file <- paste0("lightgbm.", tools::file_ext(PKG_URL))
download.file(
url = PKG_URL
, destfile = local_file
)
install.packages(
pkgs = local_file
, type = "binary"
, repos = NULL
)
library("lightgbm", lib.loc="C:/Program Files/R/R-4.0.2/library")
library(lightgbm)
data(agaricus.train, package='lightgbm')
train <- agaricus.train
dtrain <- lgb.Dataset(train$data, label=train$label)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
model
######################
## LightGBM example ##
######################
getwd()
######################
## LightGBM example ##
######################
library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
library(lightgbm)
set.seed(257)
train = fread("../input/train.csv") %>% as.data.frame()
train = fread("train.csv") %>% as.data.frame()
test  = fread("/test.csv")  %>% as.data.frame()
test  = fread("test.csv")  %>% as.data.frame()
# 결측치 --> median 치환 function
median.impute = function(x){
x = as.data.frame(x)
for (i in 1:ncol(x)){
x[which(x[,i]== -1),i] = NA
}
x = x %>% mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .)) %>% as.data.table()
return(x)
}
train = median.impute(train)
test  = median.impute(test)
test$target = NA
data        = rbind(train, test)
rowSums(data == -1, na.rm = T)
data[, fe_amount_NA := rowSums(data == -1, na.rm = T)]
data[, ps_car_13_ps_reg_03 := ps_car_13*ps_reg_03]
data[, ps_reg_mult := ps_reg_01*ps_reg_02*ps_reg_03]
#- Create LGB Dataset
varnames = setdiff(colnames(data), c("id", "target"))
varnames
setdiff
colnames(data)
varnames
data
data[!is.na(target), varnames, with = F]
train_sparse = Matrix(as.matrix(data[!is.na(target), varnames, with = F]), sparse=TRUE)
test_sparse  = Matrix(as.matrix(data[is.na(target) , varnames, with=F]), sparse=TRUE)
y_train  = data[!is.na(target),target]
test_ids = data[is.na(target) ,id]
test_ids
lgb.train = lgb.Dataset(data=train_sparse, label=y_train)
categoricals.vec = colnames(train)[c(grep("cat",colnames(train)))]
categoricals.vec
NormalizedGini
#- Setting up Gini Eval Function
lgb.normalizedgini = function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = MLmetrics::NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
#- Setting up LGBM Parameters
lgb.grid = list(objective = "binary",
metric    = "auc",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
#- Cross Validation
lgb.model.cv = lgb.cv(
params = lgb.grid,
data   = lgb.train,
learning_rate = 0.02,
num_leaves = 25,
num_threads = 2,
nrounds = 7000,
early_stopping_rounds = 50,
eval_freq = 20,
eval = lgb.normalizedgini,
categorical_feature = categoricals.vec,
nfold = 5,
stratified = TRUE)
best.iter = lgb.model.cv$best_iter
best.iter
#- Train Final Model
lgb.model = lgb.train(
params              = lgb.grid,
data                = lgb.train,
learning_rate       = 0.02,                        #- *** 훈련량
num_leaves          = 25,                          #- * 트리가 가질수 있는 최대 잎사귀 수
num_threads         = 2 ,                          #- * 병렬처리시 처리할 쓰레드
nrounds             = best.iter,                   #- *** 계속 나무를 반복하며 부스팅을 하는데 몇번을 할것인가이다. 1000이상정도는 해주도록 함
#-     early_stopping이 있으면 최대한 많이 줘도 (10,000~)별 상관이 없음
eval_freq           = 20,
eval                = lgb.normalizedgini,
categorical_feature = categoricals.vec)
test_ids
#- Create and Submit Predictions
preds = data.table(id=test_ids, target=predict(lgb.model,test_sparse))
preds
remove.packages("lightgbm")
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0/lightgbm-3.0.0-r-cran.tar.gz"
remotes::install_url(PKG_URL)
library("Rcpp", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:Rcpp", unload=TRUE)
library("cpp11", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:cpp11", unload=TRUE)
library("Rcpp", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:Rcpp", unload=TRUE)
library("cpp11", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:cpp11", unload=TRUE)
remove.packages("Rcpp")
install.packages("Rcpp")
library(DMwR);library(dplyr);library(data.table);library(caret);library(catboost);library(Matrix);library(ROCR);library(lightgbm)
setwd("C:/r/Monthly-Dacon-8th/")
source('C:/r/Monthly-Dacon-8th/monthlyDacon_8_common.R')
##################
## Data Loading ##
##################
sample_submission <- data.table::fread(
"sample_submission.csv",
stringsAsFactors = F,
data.table       = F
)
train <- data.table::fread(
"train.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
test  <- data.table::fread(
"test_x.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
revVar  <- c("QaA", "QdA", "QeA", "QfA", "QgA", "QiA", "QkA", "QnA", "QqA", "QrA")
train[revVar] <- train %>% select(revVar) %>% mutate_all(list(~6 - .))
test[revVar]  <- test %>% select(revVar) %>% mutate_all(list(~6 - .))
#- 2. machia score = 전체 점수의 평균 값 계산
machiaVar             <- train %>% select(matches("Q.A")) %>%  colnames
train$machiaScore     <- train %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
test$machiaScore      <- test  %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
#- 3 wf_mean, wr_mean, voca_mean(실제 단어를 아는 경우(wr)  - 허구인 단어를 아는 경우(wf) / 13)
wfVar <- train %>% select(matches("wf.")) %>%  colnames
wrVar <- train %>% select(matches("wr.")) %>%  colnames
#- 3.1 wf_mean
train$wf_mean <- train %>% select(wfVar) %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wf_mean  <- test %>% select(wfVar)  %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.2 wr_mean
train$wr_mean <- train %>% select(wrVar) %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wr_mean  <- test %>% select(wrVar)  %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.3 voca_mean
train$voca_mean <- train %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
test$voca_mean <- test %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
#- tp variable
tpPs <- c("tp01", "tp03", "tp05", "tp07", "tp09")
tpNg <- c("tp02", "tp04", "tp06", "tp08", "tp10")
#- 3.4 tp_positive
train$tp_positive  <- train %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_positive   <- test  %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.5 tp_negative
train$tp_negative  <- train %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_negative   <- test  %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.6 tp_mean
train$tp_mean <- train %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
test$tp_mean  <- test %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
##############################
## 변수타입설정 & 변수 선택 ##
##############################
#- 수치형 변수
num_var <- train %>%  select_if(is.numeric) %>%  colnames
#- 범주형(명목형) 변환
factor_var <- c("engnat",
"age_group",
"gender",
"hand",
"married",
"race",
"religion",
"urban",
"voted")
train[factor_var]        <- train %>% select(all_of(factor_var))        %>% mutate_all(as.factor)
test[factor_var[c(-9)]]  <-  test %>% select(all_of(factor_var[c(-9)])) %>% mutate_all(as.factor)
#- 범주형(순서형) 변환
ordered_var1 <- colnames(train)[grep("Q.A", colnames(train))]
ordered_var2 <- colnames(train)[grep("tp|wr|wf.", colnames(train))]
train[c(ordered_var1, ordered_var2)]   <- train %>% select(all_of(ordered_var1), all_of(ordered_var2)) %>% mutate_all(as.ordered)
test[c(ordered_var1, ordered_var2) ]   <- test %>% select(all_of(ordered_var1), all_of(ordered_var2)) %>% mutate_all(as.ordered)
#-  변수 제거
remv_var <- c("index")
train    <- train %>%  select(-remv_var)
test     <- test  %>%  select(-remv_var)
trainData <- train
testData  <- test
trainData_cat <- trainData
testData_cat  <- testData
YIdx       <- which(colnames(trainData_cat) %in% c('voted'))
features   <- trainData_cat[-YIdx]
labels     <- ifelse(trainData_cat[,YIdx] == 1, 0, 1)
train_pool <- catboost.load_pool(data = features, label = labels)
# 2. catboost.train 함수를 이용하여 train
set.seed(1)
model <- catboost.train(
train_pool,                                  #- 학습에 사용하고자 하는 train_pool
NULL,                                        #-
params = list(loss_function = 'Logloss',     #- loss function 지정(여기서는 분류모형이므로 Logloss)
random_seed   = 123,           #- seed number
custom_loss   = "AUC",         #- 모델링 할 때 추가로 추출할 값들 (train_dir로 지정한 곳으로 해당 결과를 파일로 내보내준다)
train_dir     = "./model/CatBoost_R_output", # 모델링 한 결과를 저장할 directory
iterations    = 1000,                         #- 학습 iteration 수
metric_period = 10)
)
# 3. catboost.predict function
real_pool  <- catboost.load_pool(testData_cat)
YHat_cat   <- catboost.predict(
model,
real_pool,
prediction_type = c('Probability'))  # Probability, Class
library(DMwR);library(dplyr);library(data.table);library(caret);library(catboost);library(Matrix);library(ROCR);library(lightgbm)
setwd("C:/r/Monthly-Dacon-8th/")
source('C:/r/Monthly-Dacon-8th/monthlyDacon_8_common.R')
##################
## Data Loading ##
##################
sample_submission <- data.table::fread(
"sample_submission.csv",
stringsAsFactors = F,
data.table       = F
)
train <- data.table::fread(
"train.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
test  <- data.table::fread(
"test_x.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
revVar  <- c("QaA", "QdA", "QeA", "QfA", "QgA", "QiA", "QkA", "QnA", "QqA", "QrA")
train[revVar] <- train %>% select(revVar) %>% mutate_all(list(~6 - .))
test[revVar]  <- test %>% select(revVar) %>% mutate_all(list(~6 - .))
#- 2. machia score = 전체 점수의 평균 값 계산
machiaVar             <- train %>% select(matches("Q.A")) %>%  colnames
train$machiaScore     <- train %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
test$machiaScore      <- test  %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
#- 3 wf_mean, wr_mean, voca_mean(실제 단어를 아는 경우(wr)  - 허구인 단어를 아는 경우(wf) / 13)
wfVar <- train %>% select(matches("wf.")) %>%  colnames
wrVar <- train %>% select(matches("wr.")) %>%  colnames
#- 3.1 wf_mean
train$wf_mean <- train %>% select(wfVar) %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wf_mean  <- test %>% select(wfVar)  %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.2 wr_mean
train$wr_mean <- train %>% select(wrVar) %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wr_mean  <- test %>% select(wrVar)  %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.3 voca_mean
train$voca_mean <- train %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
test$voca_mean <- test %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
#- tp variable
tpPs <- c("tp01", "tp03", "tp05", "tp07", "tp09")
tpNg <- c("tp02", "tp04", "tp06", "tp08", "tp10")
#- 3.4 tp_positive
train$tp_positive  <- train %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_positive   <- test  %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.5 tp_negative
train$tp_negative  <- train %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_negative   <- test  %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.6 tp_mean
train$tp_mean <- train %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
test$tp_mean  <- test %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
##############################
## 변수타입설정 & 변수 선택 ##
##############################
#- 수치형 변수
num_var <- train %>%  select_if(is.numeric) %>%  colnames
#- 범주형(명목형) 변환
factor_var <- c("engnat",
"age_group",
"gender",
"hand",
"married",
"race",
"religion",
"urban",
"voted")
train[factor_var]        <- train %>% select(all_of(factor_var))        %>% mutate_all(as.factor)
test[factor_var[c(-9)]]  <-  test %>% select(all_of(factor_var[c(-9)])) %>% mutate_all(as.factor)
#- 범주형(순서형) 변환
ordered_var1 <- colnames(train)[grep("Q.A", colnames(train))]
ordered_var2 <- colnames(train)[grep("tp|wr|wf.", colnames(train))]
#-  변수 제거
remv_var <- c("index")
train    <- train %>%  select(-remv_var)
test     <- test  %>%  select(-remv_var)
#- one-hot encoding (필요시) -- LightGBM
oneHotVar       <- c(factor_var[-9])
train_fac       <- train %>% select(all_of(oneHotVar))
dmy_model       <- caret::dummyVars("~ .", data = train_fac)
train_oneHot    <- data.frame(predict(dmy_model, train_fac))
train  <- train %>% select(-oneHotVar)
train  <- dplyr::bind_cols(train, train_oneHot)
test_fac       <- test %>% select(all_of(oneHotVar[c(-9)]))
dmy_model      <- caret::dummyVars("~ .", data = test_fac)
test_oneHot    <- data.frame(predict(dmy_model, test_fac))
test  <- test %>% select(-oneHotVar)
test  <- dplyr::bind_cols(test, test_oneHot)
rm(ls = test_oneHot)
rm(ls = train_oneHot)
rm(ls = train_fac)
rm(ls = test_fac)
## final 제출시, 적용
trainData <- train
testData  <- test
varnames     = setdiff(colnames(trainData), c("voted"))
train_sparse = Matrix(as.matrix(trainData[, varnames]), sparse=TRUE)
test_sparse  = Matrix(as.matrix(testData[,  varnames]), sparse=TRUE)
y_train = trainData[, c("voted")]
# binary, auc 계산시, 반드시 Y 값은 0 또는 1이어야 함
train.lgb <- lgb.Dataset(data  = train_sparse, label = ifelse(y_train == 2, 1, 0))
test.lgb  <- lgb.Dataset(data  = test_sparse)
categoricals.vec <- c(ordered_var1, ordered_var2)
lgb.grid = list(objective = "binary",
metric    = "auc",
#min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
#min_data = 100,
#max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
#min_data_in_bin=100,
#min_gain_to_split = 10,
#min_data_in_leaf = 30,
is_unbalance = F)
#- Setting up Gini Eval Function
lgb.normalizedgini = function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = MLmetrics::NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
#- Cross Validation
set.seed(1)
lgb.model.cv = lgb.cv(
params                = lgb.grid,
data                  = train.lgb,
learning_rate         = 0.02,                    #- *** 훈련량
#num_leaves            = 25,
num_threads           = 2,                       #- * 병렬처리시 처리할 쓰레드
nrounds               = 7000,
early_stopping_rounds = 50,                      #- ** 더이상 발전이 없으면 그만두게 설정할때 이를 몇번동안 발전이 없으면 그만두게 할지 여부
eval_freq             = 20,
eval                  = lgb.normalizedgini,
categorical_feature   = categoricals.vec,
nfold                 = 10,
stratified            = TRUE)
best.iter = lgb.model.cv$best_iter
best.iter
lgb_model = lgb.train(
params              = lgb.grid,
data                = train.lgb,
learning_rate       = 0.02,                        #- *** 훈련량
#num_leaves          = 25,                          #- * 트리가 가질수 있는 최대 잎사귀 수
num_threads         = 2,                          #- * 병렬처리시 처리할 쓰레드
nrounds             = best.iter,                   #- *** 계속 나무를 반복하며 부스팅을 하는데 몇번을 할것인가이다. 1000이상정도는 해주도록 함
#-     early_stopping이 있으면 최대한 많이 줘도 (10,000~)별 상관이 없음
eval_freq           = 20,
eval                = lgb.normalizedgini)
#- Create and Submit Predictions
YHat_lgbm       <- predict(lgb_model, test_sparse)
sample_submission$voted <- (YHat_cat + YHat_lgbm) / 2
write.csv(sample_submission, "submission_data.csv", row.names = F)
library(DMwR);library(dplyr);library(data.table);library(caret);library(catboost);library(Matrix);library(ROCR);library(lightgbm)
setwd("C:/r/Monthly-Dacon-8th/")
source('C:/r/Monthly-Dacon-8th/monthlyDacon_8_common.R')
##################
## Data Loading ##
##################
sample_submission <- data.table::fread(
"sample_submission.csv",
stringsAsFactors = F,
data.table       = F
)
train <- data.table::fread(
"train.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
test  <- data.table::fread(
"test_x.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
revVar  <- c("QaA", "QdA", "QeA", "QfA", "QgA", "QiA", "QkA", "QnA", "QqA", "QrA")
train[revVar] <- train %>% select(revVar) %>% mutate_all(list(~6 - .))
test[revVar]  <- test %>% select(revVar) %>% mutate_all(list(~6 - .))
#- 2. machia score = 전체 점수의 평균 값 계산
machiaVar             <- train %>% select(matches("Q.A")) %>%  colnames
train$machiaScore     <- train %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
test$machiaScore      <- test  %>% select(machiaVar) %>% transmute(machiaScore = rowMeans(across(where(is.numeric)))) %>% unlist %>% as.numeric
#- 3 wf_mean, wr_mean, voca_mean(실제 단어를 아는 경우(wr)  - 허구인 단어를 아는 경우(wf) / 13)
wfVar <- train %>% select(matches("wf.")) %>%  colnames
wrVar <- train %>% select(matches("wr.")) %>%  colnames
#- 3.1 wf_mean
train$wf_mean <- train %>% select(wfVar) %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wf_mean  <- test %>% select(wfVar)  %>% transmute(wf_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.2 wr_mean
train$wr_mean <- train %>% select(wrVar) %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
test$wr_mean  <- test %>% select(wrVar)  %>% transmute(wr_mean = round(rowMeans(across(where(is.numeric))), 8)) %>% unlist %>% as.numeric
#- 3.3 voca_mean
train$voca_mean <- train %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
test$voca_mean <- test %>% transmute(voca_mean = round((wr_01 + wr_02 + wr_03 + wr_04 + wr_05 + wr_06 + wr_07 + wr_08 + wr_09 + wr_10 + wr_11 + wr_12 + wr_13 - wf_01 - wf_02 - wf_03 / 16), 8)) %>% unlist %>% as.numeric
#- tp variable
tpPs <- c("tp01", "tp03", "tp05", "tp07", "tp09")
tpNg <- c("tp02", "tp04", "tp06", "tp08", "tp10")
#- 3.4 tp_positive
train$tp_positive  <- train %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_positive   <- test  %>% select(tpPs) %>% transmute(tp_positive = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.5 tp_negative
train$tp_negative  <- train %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
test$tp_negative   <- test  %>% select(tpNg) %>% transmute(tp_negative = round(rowMeans(across(where(is.numeric))), 8)) %>%  unlist %>% as.numeric
#- 3.6 tp_mean
train$tp_mean <- train %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
test$tp_mean  <- test %>% transmute(tp_mean = round(((tp01 + tp03 + tp05 + tp07 + tp09 + (6 - tp02) + (6 - tp04) + (6 - tp06) + (6 - tp08) + (6 - tp10)) / 10), 8)) %>%  unlist %>% as.numeric
num_var <- train %>%  select_if(is.numeric) %>%  colnames
#- 범주형(명목형) 변환
factor_var <- c("engnat",
"age_group",
"gender",
"hand",
"married",
"race",
"religion",
"urban",
"voted")
train[factor_var]        <- train %>% select(all_of(factor_var))        %>% mutate_all(as.factor)
test[factor_var[c(-9)]]  <-  test %>% select(all_of(factor_var[c(-9)])) %>% mutate_all(as.factor)
#- 범주형(순서형) 변환
ordered_var1 <- colnames(train)[grep("Q.A", colnames(train))]
ordered_var2 <- colnames(train)[grep("tp|wr|wf.", colnames(train))]
train[c(ordered_var1, ordered_var2)]   <- train %>% select(all_of(ordered_var1), all_of(ordered_var2)) %>% mutate_all(as.ordered)
test[c(ordered_var1, ordered_var2) ]   <- test %>% select(all_of(ordered_var1), all_of(ordered_var2)) %>% mutate_all(as.ordered)
remv_var <- c("index")
train    <- train %>%  select(-remv_var)
test     <- test  %>%  select(-remv_var)
## final 제출시, 적용
trainData <- train
testData  <- test
rm(ls = train)
rm(ls = test)
trainData_cat <- trainData
testData_cat  <- testData
YIdx       <- which(colnames(trainData_cat) %in% c('voted'))
features   <- trainData_cat[-YIdx]
labels     <- ifelse(trainData_cat[,YIdx] == 1, 0, 1)
train_pool <- catboost.load_pool(data = features, label = labels)
# 2. catboost.train 함수를 이용하여 train
set.seed(1)
model <- catboost.train(
train_pool,                                  #- 학습에 사용하고자 하는 train_pool
NULL,                                        #-
params = list(loss_function = 'Logloss',     #- loss function 지정(여기서는 분류모형이므로 Logloss)
random_seed   = 123,           #- seed number
custom_loss   = "AUC",         #- 모델링 할 때 추가로 추출할 값들 (train_dir로 지정한 곳으로 해당 결과를 파일로 내보내준다)
train_dir     = "./model/CatBoost_R_output", # 모델링 한 결과를 저장할 directory
iterations    = 1000,                         #- 학습 iteration 수
metric_period = 10)
)
real_pool  <- catboost.load_pool(testData_cat)
YHat_cat   <- catboost.predict(
model,
real_pool,
prediction_type = c('Probability'))  # Probability, Class
YHat_cat
