#######################
# 1. caret model List #
#######################
caretModelList <- caret::modelLookup()
caretModelList
caretModelList$model[grep("nnet", caretModelList$model)]
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = 'rf',
metrics     = 'Accuracy',
# tuneGrid    = expand.grid(mtry = c(3)),
trControl   = trainControl(method = 'none', number = 3, repeats = 3, verboseIter = T)
)
modelResult
modelResult
modelResult$bestTune
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = 'rf',
metrics     = 'Accuracy'
)
caretModelList$model[grep("svmLinear", caretModelList$model)]
caretModelList$model[grep("XGTree", caretModelList$model)]
caretModelList$model[grep("XG", caretModelList$model)]
caretModelList$model[grep("xg", caretModelList$model)]
method <- 'rf'
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = method,
metrics     = 'Accuracy'
)
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = method,
metrics     = 'Accuracy',
caret::trainControl(method = 'none')
)
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = method,
metrics     = 'Accuracy',
trControl   =  caret::trainControl(method = 'none')
)
modelResult
YHat <- predict.train(
object = modelResult, # caret model 객체
newdata = testData, # 예측하고자 하는 data
type = c('raw') # 예측 타입
)
result <- caret::confusionMatrix(
YHat,
testData$grade
)
result
result$table
result$overall
result
result <- caret::confusionMatrix(
YHat,
testData$grade,
mode = "prec_recall"
)
result
result$table
result$overall
result$byClass
result$byClass["F1"]
result$byClass
str(result$byClass)
result$byClass[7]
result$byClass[,7]
data.frame("method" = method, "f1_score" = result$byClass[,7])
result$byClass[,7]
c(method, result$byClass[,7])
data.frame(c(method, result$byClass[,7]))
data.frame("method" = method, "f1_score" = result$byClass[,7])
# method <- 'rf'
eval_df <- data.frame()
# method <- 'rf'
eval_df <- data.frame("method" = NULL, "f1_score" = NULL)
eval_df
# method <- 'rf'
eval_df <- data.frame("method" = NA, "f1_score" = NA)
eval_df
for(method in c('rf', 'svmLinear', 'nnet', 'xgbTree')){
modelResult <- train(
formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd),
data        = trainData,
method      = method,
metrics     = 'Accuracy',
trControl   =  caret::trainControl(method = 'none')
)
YHat <- predict.train(
object = modelResult, # caret model 객체
newdata = testData, # 예측하고자 하는 data
type = c('raw') # 예측 타입
)
result <- caret::confusionMatrix(
YHat,
testData$grade,
mode = "prec_recall"
)
rbind(eval_df, data.frame("method" = method, "f1_score" = result$byClass[,7]))
}
modelResult
# 0. 공통 변수 생성
f =    formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd)
# 0. 공통 변수 생성
f = formula(grade ~ duration  + count + amount + durationGrd + amtPerCnt + customerGrd)
# 1. randomForest 생성
modelResult <- train(
f,
data        = trainData,
method      = 'rf',
metrics     = 'Accuracy',
trControl   =  caret::trainControl(method = 'none')
)
YHat <- predict.train(
object  = modelResult,    #- caret model 객체
newdata = testData,       #- 예측하고자 하는 data
type    = c('raw')        #- 예측 타입
)
result <- caret::confusionMatrix(
YHat,
testData$grade,
mode = "prec_recall"
)
result
#######################
# 5. 모델 결과 시각화 #
#######################
# 수치 예측 PLOT
plot2 <- data.frame('Y'     = Y,
'YHat'  = Yhat,
'resid' = NA,
'index' = 1:nrow(modelResult$trainingData))
ROCR::prediction(
YHat,  # 예측 값
testData$grade         # 실제 값
)
YHat
testData$grade
library(ROCR)
ROCR::prediction(
YHat,                  # 예측 값
testData$grade         # 실제 값
)
YHat
prediction
YHat
ROCR::prediction
multiclass.roc
ibrary(pROC)
install.packages("pROC")
install.packages("pROC")
library(pROC)
pROC::multiclass.roc(
YHat,                  # 예측 값
testData$grade         # 실제 값
)
multiclass.roc
as.ordered(y_pred)
as.ordered(YHat)
YHat
pROC::multiclass.roc(
as.ordered(YHat),                  # 예측 값
testData$grade         # 실제 값
)
pROC::multiclass.roc(
as.ordered(YHat),                  # 예측 값
as.ordered(testData$grade)         # 실제 값
)
auc <- pROC::multiclass.roc(
as.ordered(YHat),                  # 예측 값
as.ordered(testData$grade)         # 실제 값
)
auc
print(auc)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
plot(auc)
auc <- pROC::multiclass.roc(
as.ordered(YHat),                  # 예측 값
as.ordered(testData$grade)         # 실제 값
levels = c(1, 2, 3, 4, 5)
)
auc <- pROC::multiclass.roc(
as.ordered(YHat),                  # 예측 값
as.ordered(testData$grade),         # 실제 값
levels = c(1, 2, 3, 4, 5)
)
auc
print(auc)
plot(auc)
data(iris)
library(randomForest)
library(pROC)
set.seed(1000)
# 3-class in response variable
rf = randomForest(Species~., data = iris, ntree = 100)
rf
# predict(.., type = 'prob') returns a probability matrix
multiclass.roc(iris$Species, predict(rf, iris, type = 'prob'))
# predict(.., type = 'prob') returns a probability matrix
roc <- multiclass.roc(iris$Species, predict(rf, iris, type = 'prob'))
roc
install.packages("AppliedPredictiveModeling")
apropos
# apropos function
# 관심있는 class나 함수를 찾는데 유용한 함수
apropos("confusion")
library(AppliedPredictiveModeling)
# apropos function
# 관심있는 class나 함수를 찾는데 유용한 함수
apropos("confusion")
# 모든 패키지 중에서 함수를 찾아야 할 경우
# RSiteSearch function
# 모든 패키지 중에서 함수를 찾아야 할 경우
RSiteSearch("confusion", restrict = "functions")
data("segmentationOriginal")
segData <- subset(segmentationOriginal, Case == "Train")
segData
colnames(segData)
cellID <- segData$Cell
cellID
class  <- segData$Class
case   <- segData$Case
segData <- segData[, -c(1:3)]
# 원본 데이터에는 예측 변수의 바이너리 형태인 여러 "상태(status)" 관련 컬럼이 포함돼 있음
# 이 컬럼들을 제거하려면 컬럼 이름에 "Status"가 포함된 컬럼을 찾아야 함. 이들을 제거하자
grep("Status", colnames(segData))
# 원본 데이터에는 예측 변수의 바이너리 형태인 여러 "상태(status)" 관련 컬럼이 포함돼 있음
# 이 컬럼들을 제거하려면 컬럼 이름에 "Status"가 포함된 컬럼을 찾아야 함. 이들을 제거하자
statusColNum <- grep("Status", colnames(segData))
segData <- segData[,-statusColNum]
##########
## 변환 ##
##########
library(e1071)
skewValues <- apply(segData, 2, skewness)
skewValues
head(skewValues)
# caret package의 BoxCoxTrans function을 통해 적절한 변환법을 찾은 후,
# 이 방법을 적용해 새 데이터를 만들어 줌
library(caret)
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
predict(Ch1AreaTrans, head(segData$AreaCh1))
ion을 이용하여 PCA 적용
# 중심화 및 척도화 진행
# prcomp function을 이용하여 PCA 적용
# 중심화 및 척도화 진행
prcomp(
segData,
center = T,
scale  = T
)
# prcomp function을 이용하여 PCA 적용
# 중심화 및 척도화 진행
pcaObject <- prcomp(
segData,
center = T,
scale  = T
)
pcaObject
pcaObject$sdev^2
pcaObject$sdev^2 / sum(pcaObject$sdev^2)*100
pcaObject$sdev^2 / sum(pcaObject$sdev^2)*100
pcaObject$sd^2 / sum(pcaObject$sdev^2)*100
pcaObject$sd^2 / sum(pcaObject$sd^2)*100
sum(pcaObject$sd^2)*100
pcaObject$sd^2
percentVariance <- pcaObject$sd^2 / sum(pcaObject$sd^2)*100
percentVariance
percentVariance[1:3]
# 변환된 값은 x라고 불리는 하위 객체 형태로 저장
pcaObject$x[, 1:5]
# 변환된 값은 x라고 불리는 하위 객체 형태로 저장
head(pcaObject$x[, 1:5])
head(pcaObject$rotation[,1:3])
# 다양한 변환 #
###############
# caret::preProcess 사용
###############
# 다양한 변환 #
###############
# caret::preProcess 사용
trans <- preProcess(
segData,
method = c("BoxCox", "center", "scale", "pca"))
predict(trans, segData)
transformed <- predict(trans, segData)
head(transformed)
head(transformed[, c(1:5)])
nearZeroVar
# 예측 변수 간 상관관계를 기준으로 변수 filtering
correlations <- cor(segData)
correlations
dim(nearZeroVar)
dim(correlations)
correlations[1:4, 1:4]
corrplot::corrplot(correlations, order = 'hclust')
# findCorrelation function
# 3.5장에 나오는 상관변수 filtering algorithm 적용
findCorrelation(correlations, cutoff = 0.7)
# findCorrelation function
# 3.5장에 나오는 상관변수 filtering algorithm 적용
# 대상 예측 변수 후보를 고른 후, 해당 열의 번호 반환
highCor <- findCorrelation(correlations, cutoff = 0.7)
length(highCor)
filteredSegData <- segData[,-highCor]
filteredSegData
#- 1. https://github.com/Microsoft/LightGBM/tree/master/R-package
#- 해당 사이트에 설명에 의하면, LightGBM을 CRAN에서는 설치가 불가능하지만, 3.0.0 version 부터는 CMake, Visual Studio의 설치 필요없이
#- 설치가 가능하다고 한다.
#- 설치 방법은 다음과 같다.
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0/lightgbm-3.0.0-r-cran.tar.gz"
remotes::install_url(PKG_URL)
#- 1. https://github.com/Microsoft/LightGBM/tree/master/R-package
#- 해당 사이트에 설명에 의하면, LightGBM을 CRAN에서는 설치가 불가능하지만, 3.0.0 version 부터는 CMake, Visual Studio의 설치 필요없이
#- 설치가 가능하다고 한다.
#- 설치 방법은 다음과 같다.
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0/lightgbm-3.0.0-r-cran.tar.gz"
remotes::install_url(PKG_URL)
#- 1. https://github.com/Microsoft/LightGBM/tree/master/R-package
#- 해당 사이트에 설명에 의하면, LightGBM을 CRAN에서는 설치가 불가능하지만, 3.0.0 version 부터는 CMake, Visual Studio의 설치 필요없이
#- 설치가 가능하다고 한다.
#- 설치 방법은 다음과 같다.
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0/lightgbm-3.0.0-r-cran.tar.gz"
remotes::install_url(PKG_URL)
R.version
R.version
R.version
R.version
installr::check.for.updates.R()
installr::install.R()
updateR()
library(installr)
installr::updateR()
installr::updateR()
library(installr)
installr::updateR()
install.packages(c("clusterGeneration", "htmlwidgets", "igraph", "ps", "psych", "questionr", "RcppArmadillo", "readr", "recipes", "rJava", "rlang", "roxygen2", "rpart.plot", "RSQLite", "sandwich", "seriation", "sp", "SQUAREM", "stringi", "survival", "sys", "tibble", "tidyr", "TTR", "usethis", "vcd", "vctrs", "withr", "xfun", "XML", "xts"))
##############
## LightGBM ##
##############
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0rc1/lightgbm-3.0.0-1-r40-windows.zip"
local_file <- paste0("lightgbm.", tools::file_ext(PKG_URL))
download.file(
url = PKG_URL
, destfile = local_file
)
install.packages(
pkgs = local_file
, type = "binary"
, repos = NULL
)
library("lightgbm", lib.loc="C:/Program Files/R/R-4.0.2/library")
library(lightgbm)
data(agaricus.train, package='lightgbm')
train <- agaricus.train
dtrain <- lgb.Dataset(train$data, label=train$label)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
model
######################
## LightGBM example ##
######################
getwd()
######################
## LightGBM example ##
######################
library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
library(lightgbm)
set.seed(257)
train = fread("../input/train.csv") %>% as.data.frame()
train = fread("train.csv") %>% as.data.frame()
test  = fread("/test.csv")  %>% as.data.frame()
test  = fread("test.csv")  %>% as.data.frame()
# 결측치 --> median 치환 function
median.impute = function(x){
x = as.data.frame(x)
for (i in 1:ncol(x)){
x[which(x[,i]== -1),i] = NA
}
x = x %>% mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .)) %>% as.data.table()
return(x)
}
train = median.impute(train)
test  = median.impute(test)
test$target = NA
data        = rbind(train, test)
rowSums(data == -1, na.rm = T)
data[, fe_amount_NA := rowSums(data == -1, na.rm = T)]
data[, ps_car_13_ps_reg_03 := ps_car_13*ps_reg_03]
data[, ps_reg_mult := ps_reg_01*ps_reg_02*ps_reg_03]
#- Create LGB Dataset
varnames = setdiff(colnames(data), c("id", "target"))
varnames
setdiff
colnames(data)
varnames
data
data[!is.na(target), varnames, with = F]
train_sparse = Matrix(as.matrix(data[!is.na(target), varnames, with = F]), sparse=TRUE)
test_sparse  = Matrix(as.matrix(data[is.na(target) , varnames, with=F]), sparse=TRUE)
y_train  = data[!is.na(target),target]
test_ids = data[is.na(target) ,id]
test_ids
lgb.train = lgb.Dataset(data=train_sparse, label=y_train)
categoricals.vec = colnames(train)[c(grep("cat",colnames(train)))]
categoricals.vec
NormalizedGini
#- Setting up Gini Eval Function
lgb.normalizedgini = function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = MLmetrics::NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
#- Setting up LGBM Parameters
lgb.grid = list(objective = "binary",
metric    = "auc",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
#- Cross Validation
lgb.model.cv = lgb.cv(
params = lgb.grid,
data   = lgb.train,
learning_rate = 0.02,
num_leaves = 25,
num_threads = 2,
nrounds = 7000,
early_stopping_rounds = 50,
eval_freq = 20,
eval = lgb.normalizedgini,
categorical_feature = categoricals.vec,
nfold = 5,
stratified = TRUE)
best.iter = lgb.model.cv$best_iter
best.iter
#- Train Final Model
lgb.model = lgb.train(
params              = lgb.grid,
data                = lgb.train,
learning_rate       = 0.02,                        #- *** 훈련량
num_leaves          = 25,                          #- * 트리가 가질수 있는 최대 잎사귀 수
num_threads         = 2 ,                          #- * 병렬처리시 처리할 쓰레드
nrounds             = best.iter,                   #- *** 계속 나무를 반복하며 부스팅을 하는데 몇번을 할것인가이다. 1000이상정도는 해주도록 함
#-     early_stopping이 있으면 최대한 많이 줘도 (10,000~)별 상관이 없음
eval_freq           = 20,
eval                = lgb.normalizedgini,
categorical_feature = categoricals.vec)
test_ids
#- Create and Submit Predictions
preds = data.table(id=test_ids, target=predict(lgb.model,test_sparse))
preds
remove.packages("lightgbm")
PKG_URL <- "https://github.com/microsoft/LightGBM/releases/download/v3.0.0/lightgbm-3.0.0-r-cran.tar.gz"
remotes::install_url(PKG_URL)
library("Rcpp", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:Rcpp", unload=TRUE)
library("cpp11", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:cpp11", unload=TRUE)
library("Rcpp", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:Rcpp", unload=TRUE)
library("cpp11", lib.loc="C:/Program Files/R/R-4.0.2/library")
detach("package:cpp11", unload=TRUE)
remove.packages("Rcpp")
install.packages("Rcpp")
library(DMwR);library(dplyr);library(data.table);library(caret);library(catboost);library(Matrix);library(ROCR);library(lightgbm)
setwd("C:/r/Monthly-Dacon-8th/")
source('C:/r/Monthly-Dacon-8th/monthlyDacon_8_common.R')
##################
## Data Loading ##
##################
sample_submission <- data.table::fread(
"sample_submission.csv",
stringsAsFactors = F,
data.table       = F
)
train <- data.table::fread(
"train.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
test  <- data.table::fread(
"test_x.csv",
stringsAsFactors = F,
data.table = F,
na.strings = c("NA", "NaN", "NULL", "\\N"))
